---
{"dg-publish":true,"permalink":"/etc/__/study/think-bayes/chapter04-Estimating-Proportions/","dgPassFrontmatter":true,"noteIcon":"","created":"2023-12-20T00:33:04.000+09:00"}
---

#think-bayes #probability #study

---

이전 장에서 101그릇 문제를 풀었는데, 쿠키가 어느 그릇에서 나왔는지 맞추는 것이 아니라 비율을 추정했다.

이번 장에서는 The Euro Problem 문제를 풀면서 베이지안 통계로 한 걸음 더 나아간다. 동일한 이전 분포로 시작하여 업데이트가 동일하다는 것을 알 수 있다. 하지만 철학적으로는 다른 문제라고 주장하고, 이 문제를 통해 베이지안 통계의 두 가지 정의 요소, 즉 **사전 분포 선택**과 **미지수를 표현하기 위해 확률을 사용하는 방법**을 소개한다.

# The Euro Problem
```
정보 이론, 추론 및 학습 알고리즘에서 데이비드 맥케이는 이 문제를 제기합니다

"2002년 1월 4일 금요일 가디언 지에 한 통계가 실렸습니다

벨기에 1유로 동전을 가장자리에서 250번 돌렸을 때 앞면이 140번, 뒷면이 110번 나왔다는 것입니다. 런던 정경대학의 통계학 강사인 배리 블라이트는 '매우 의심스러워 보인다'고 말했습니다. "동전이 편향되지 않았다면 이렇게 극단적인 결과가 나올 확률은 7% 미만일 것입니다.  
  
"하지만 [맥케이는] 이 데이터가 코인이 공정하지 않고 편향되어 있다는 증거를 제공합니까?"라고 묻습니다.
```

이 질문에 답하기 위해 두 단계로 진행한다. 먼저 이항 분포(binomial distribution)를 사용하여 7%가 어디에서 나왔는지 살펴본 다음, 베이즈 정리를 사용하여 이 동전이 1등으로 나올 확률을 추정해 본다.

# The Binomial Distribution
당신에게 동전이 "공평하다"고, 즉 앞면이 나올 확률이 50%라고 가정해보자. 두 번 돌리면 네 가지 결과 `HH`, `HT`, `TH` `TT`가 나온다. 네 가지 결과 모두 25%의 동일한 확률을 갖는다.

총 앞면 수를 합하면 0, 1 또는 2의 세 가지 가능한 결과가 있습니다. 0과 2의 확률은 25%이고 1의 확률은 50%입니다.

더 일반적으로, 앞면이 나올 확률이 p 이고 동전을 n번 돌렸다고 가정해 본다. 총 k 개 앞면이 나올 확률은 이항 분포에 의해 계산된다.
$$\binom{n}{k} p^k (1-p)^{n-k}$$

또는 두 값을 모두 포함하여 0에서 n 까지의 모든 k 값이다. $\binom{n}{k}$는 이항 계수이며, 일반적으로 "n 선택 k"로 발음한다.
  
이 표현식을 직접 평가할 수도 있지만 SciPy 의 binom.pmf를 사용할 수도 있다. 예를 들어 동전을 n=2번 던지고 앞면이 나올 확률이 p=0.5인 경우, 앞면이 나올 확률은 k=1 이다.

```python
from scipy.stats import binom

n = 2
p = 0.5
k = 1

binom.pmf(k, n, p)
0.5
```

k 에 단일 값을 사용하는대신 여러 값의 배열을 사용하여 binom.pmf 를 사용할 수 있다.

```python
>>> import numpy as np
>>> ks = np.arange(n+1)
>>> ps = binom.pmf(ks, n, p)
>>> ps
array([0.25, 0.5 , 0.25])
```

empiricaldis.Pmf 로 동일하게 계산할 수 있다.

```python
>>> from empiricaldist import Pmf
>>> 
>>> pmf_k = Pmf(ps, ks)
>>> pmf_k
0    0.25
1    0.50
2    0.25
Name: , dtype: float64
```

다음 함수는 주어진 n과 p 값에 대한 이항 분포를 계산하고 그 결과를 나타내는 Pmf를 반환한다.

```python
def make_binomial(n, p):
    """Make a binomial Pmf."""
    ks = np.arange(n+1)
    ps = binom.pmf(ks, n, p)
    return Pmf(ps, ks)
```

n=250 & p=0.5 인 pmf 는 다음과 같이 계산할 수 있다.

```python
pmf_k = make_binomial(n=250, p=0.5)
```
![](https://i.imgur.com/U7SzYPD.png)

이 분포에서 가장 높은 pmf 의 quantity 은 125이다.
```python
>>> pmf_k.max_prob()
125
```

그러나 가장 가능성이 높은 수량이지만 정확히 125번 앞면이 나올 확률은 약 5%에 불과하다.

```python
>>> pmf_k[125]
0.05041221314731537
>>> pmf_k[140]
0.008357181724917673
```

140 번 앞면이 나올 확률은 더더욱 낮다.
맥케이가 인용한 기사에서 통계학자는 "코인이 편향되지 않았다면 이와 같은 극단적인 결과가 나올 확률은 7% 미만일 것"이라고 한다.
  
이항 분포를 사용하여 그가 맞는지 확인할 수 있다. 다음 함수는 PMF를 사용하여 임계값(threshold)보다 크거나 같은 총 확률을 계산한다.

```python
def prob_ge(pmf, threshold):
    """Probability of quantities greater than threshold."""
    ge = (pmf.qs >= threshold)
    total = pmf[ge].sum()
    return total
```

```python
>>> prob_ge(pmf_k, 140)
0.033210575620022706
# Pmf는 동일한 계산을 수행하는 메서드를 제공합니다.
>>> pmf_k.prob_ge(140)
0.033210575620022706
```

약 3.3%로 기사에서 말한 7%보다 낮다. 차이가 나는 이유는 통계학자가 110보다 작거나 같은 결과를 포함하는 "극단적인" 140의 모든 결과를 포함하기 때문이다.
  
그 이유를 알기 위해 예상 앞면 수가 125이라는 점을 생각해보세요. 140이 나왔다면 예상치를 15 초과했다. 그리고 110이 나오면 15가 부족하다. 
  
다음 그림과 같이 7%는 이 두 '꼬리'의 합이다.
![](https://i.imgur.com/rQE3Hqw.png)

왼쪽 꼬리의 확률을 계산하는 방법은 다음과 같다.
```python
>>> pmf_k.prob_le(110)
0.033210575620022706
```

110보다 작거나 같은 결과가 나올 확률도 3.3%이므로 140과 같은 "극단적인" 결과가 나올 총 확률은 6.6% 이다.
  
이 계산의 요점은 동전이 공정하다면 이러한 극단적인 결과가 발생할 가능성이 낮다는 것이다. 흥미롭지만 맥케이의 질문에 대한 답은 아니므로 좀 더 살펴보자.

# Bayesian Estimation
어떤 동전이든 가장자리로 돌렸을 때 앞면이 위로 나올 확률은 어느 정도 있으며, 이 확률을 x라고 하자. x는 무게 분포와 같은 동전의 물리적 특성에 따라 달라진다고 생각해보자.  동전이 완벽하게 균형 잡힌 경우 x는 50% 에 가까울 것으로 예상하지만, 한쪽으로 치우친 동전의 경우 x는 크게 달라질 수 있다. 베이즈 정리와 관찰된 데이터를 사용하여 x를 추정할 수 있다.  
  
간단하게 설명하기 위해 모든 x 값이 똑같이 확률적이라고 가정하는 균등 선행(uniform prior)으로 시작한다.

```python
>>> hypos = np.linspace(0, 1, 101)
>>> prior = Pmf(1, hypos)
```

hypos는 0과 1 사이의 동일한 간격의 값 배열이다. 가설을 이용해 다음과 같이 likelihood 을 계산할 수 있다.

```python
>>> likelihood_heads = hypos
>>> likelihood_tails = 1 - hypos
>>> likelihood = {
    'H': likelihood_heads,
    'T': likelihood_tails
}
# H가 140회 반복되고 T가 110회 반복되는 문자열을 만듭니다.
>>> dataset = 'H' * 140 + 'T' * 110
```

```python
>>> def update_euro(pmf, dataset):
	"""Update pmf with a given sequence of H and T."""
	for data in dataset:
		pmf *= likelihood[data]

	pmf.normalize()
```

- 첫 번째 인수는 이전을 나타내는 Pmf
- 두 번째 인수는 문자열 시퀀스
	- 루프를 통과할 때마다 pmf에 한 가지 결과의 likelihood(앞면은 H, 뒷면은 T)을 곱한다.
- 정규화는 루프 외부에 있으므로 마지막에 한 번만 정규화한다.
	- 이는 매번 스핀할 때마다 정규화하는 것보다 더 효율적이다.  
  
update_euro 함수를 사용하는 방법은 다음과 같다.
```python
>>> posterior = prior.copy()
>>> update_euro(posterior, dataset)
```

![](https://i.imgur.com/Xb4MTlg.png)

- 이 그림은 우리가 관찰한 동전의 앞면 비율인 x의 사후분포이다.
- 사후 분포는 데이터를 본 후 x에 대한 우리의 믿음(belief)을 나타낸다. 0.4보다 작고 0.7보다 큰 값은 가능성이 낮고 0.5와 0.6 사이의 값이 가장 가능성이 높다.
  
실제로 x에 대해 가장 가능성이 높은 값은 0.56(140/250) 이다.

# Triangle Prior
- 지금까지는 uniform 사전분포를 사용했다.
- 하지만 이는 동전에 대해 우리가 알고 있는 지식에 비추어 볼 때 합리적인 선택이 아닐 수도 있다. 동전이 한쪽으로 치우치면 x가 0.5에서 크게 벗어날 수 있다고 생각할 수 있지만, 벨기에 유로 동전의 경우 x가 0.1 또는 0.9가 될 정도로 불균형할 가능성은 거의 없다.
- 0.5에 가까운 x 값에 더 높은 확률을 부여하고 극단적인 값에 더 낮은 확률을 부여하는 prior 을 선택하는 것이 더 합리적일 수 있다.
  
예를 들어 삼각형 모양의 prior 을 사용해보자.
```python
ramp_up = np.arange(50)
ramp_down = np.arange(50, -1, -1)

a = np.append(ramp_up, ramp_down)

triangle = Pmf(a, hypos, name='triangle')
triangle.normalize()
```

![](https://i.imgur.com/nLCBG2Z.png)

```python
update_euro(uniform, dataset)
update_euro(triangle, dataset)
```

![](https://i.imgur.com/xgstgym.png)

- 사후 분포의 차이는 거의 눈에 띄지 않을 정도로 작다.
- good news!
	- 두 사람이 균등 분포와 삼각형 분포 중 어느 것이 더 나은지에 대해 격렬하게 동의하지 않는다고 상상해보자. 각자 선호하는 방식에 대한 이유가 있지만, 어느 누구도 상대방의 마음을 바꾸도록 설득할 수는 없다. 왜냐, 사후분포의 큰 차이가 없으니까!
- 하지만 두 사람이 데이터를 사용하여 자신의 신념을 업데이트하는 데 동의한다고 가정해 보자. 두 사람의 사후 분포를 비교하면 논쟁의 여지가 거의 없다.
- 이것은 선행값의 늪(swamping the priors)에 빠지는 예입니다. **데이터가 충분하면 서로 다른 선행값으로 시작한 사람들이 동일한 사후 분포로 수렴하는 경향**이 있다.

# The Binomial Likelihood Function
지금까지는 한 번에 한 번씩 업데이트했기 때문에 유로화 문제의 경우 250번의 업데이트를 수행해야 했다.  
  
더 효율적인 대안은 전체 데이터 집합의 가능성을 한 번에 계산하는 것이다. 각 x에 대해 250번 던졌을 때 140개 앞면 나올 확률을 계산해야 한다.
  
이항 분포가 이 질문에 대한 답을 제시한다. 앞면이 나올 확률이 p 라면, n 번 동안 k 번 앞면 나올 확률은 다음과 같다.

$$\binom{n}{k} p^k (1-p)^{n-k}$$

# Bayesian Statistics
유로 문제와 101 그릇 문제 사이의 유사점을 눈치챘을 것이다. 앞의 분포도 같고, 확률도 같으며, 같은 데이터를 사용하면 결과도 같을 것이다. 하지만 두 가지 차이점이 있다.
  
첫 번째는 prior 선택이다. 101개의 그릇이 있을 때, 균등 선행은 문제의 문장에 의해 암시되는데, 이는 그릇 중 하나를 동일한 확률로 무작위로 선택한다.
  
유로 문제에서 **prior 선택은 주관적이다**. 즉, 동전에 대한 정보가 다르거나 동일한 정보를 다르게 해석하기 때문에 사람들은 동의하지 않을 수 있다.  
  
prior 이 주관적이기 때문에 posterior 도 주관적이다. (어떤 사람들은 이를 문제제기하기도 한다.)
  
또 다른 차이점은 우리가 추정하는 것의 성격(nature)입니다. 101개의 그릇 문제에서는 그릇을 무작위로 선택하기 때문에 각 그릇을 선택할 확률을 계산하는 것이 논란의 여지가 없다. 유로 문제에서 앞면 비율은 주어진 동전의 물리적 속성이다. 확률에 대한 일부 해석에 따르면 물리적 속성은 무작위로 간주되지 않기 때문에 문제가 된다.
  
예를 들어 우주의 나이를 생각해 보자. 현재 가장 좋은 추정치는 138억 8천만 년이지만, 어느 방향이든 0.02억 년 정도 오차가 있을 수 있다 ([여기를 참조](https://en.wikipedia.org/wiki/Age_of_the_universe)).  
  
이제 우주의 나이가 실제로 138억1천만 년보다 더 클 확률을 알고 싶다고 가정해 보자. 확률에 대한 일부 해석에 따르면 우리는 그 질문에 답할 수 없다. "우주의 나이는 무작위 수량이 아니므로 특정 값을 초과할 확률은 없다."라고 말해야 한다.

베이지안 해석에 따르면, 물리량을 무작위처럼 취급하고 그에 대한 확률을 계산하는 것이 의미 있고 유용하다.

유로화 문제에서 사전 분포는 일반적으로 동전에 대해 우리가 믿는 것을 나타내고, 사후 분포는 데이터를 본 후 특정 동전에 대해 믿는 것을 나타낸다. 따라서 사후 분포를 사용하여 동전과 그 동전의 앞면 비율에 대한 확률을 계산할 수 있습니다.  
  
**앞의 주관성과 뒤 해석은 베이즈 정리를 사용하는 것과 베이지안 통계를 사용하는 것의 주요 차이점이다**.
  
**베이즈 정리는 확률에 관한 수학적 법칙**이다. 하지만 베이지안 통계는 의외로 논란의 여지가 많다. 역사적으로 많은 사람들이 베이지안 통계의 주관성과 무작위가 아닌 것에 확률을 사용하는 것에 대해 문제를 제기해 왔다.
  
이 역사에 관심이 있으시다면 샤론 버츠 맥그레이의 저서 '[죽지 않는 이론](https://yalebooks.yale.edu/book/9780300188226/theory-would-not-die)'을 추천한다.

# Summary
- 이 장에서는 데이비드 맥케이의 유로화 문제를 제시하고 풀었다.
	- 데이터가 주어졌을 때, 유로 동전이 1등으로 나올 확률인 x의 사후 분포를 계산다.
- 두 가지 다른 선행 분포를 가정하고 동일한 데이터로 업데이트한 결과, 후행 분포가 거의 동일하다는 것을 발견했다.
	- 이는 두 사람이 서로 다른 신념으로 시작하여 동일한 데이터를 보게 되면 신념이 수렴하는 경향이 있음을 밝혔다.
- 이 장에서는 사후 분포를 보다 효율적으로 계산하기 위해 사용한 이항 분포에 대해 살펴보았다.
- 101개 그릇 문제에서와 같이 베이즈 정리를 적용하는 것과 유로 문제에서와 같이 베이지안 통계를 사용하는 것의 차이점에 대해 설명했다.  
- 하지만 여전히 맥케이의 질문에 답하지 못했다
	- "이 데이터는 동전이 공정하지 않고 편향되어 있다는 증거를 제공합니까?"
	- 이 질문은 조금 더 두고 <<테스팅>>에서 다시 다룬다.
